# Notes

- Causal Attention and Multi Head Attention

- In the MultiHeadAttentionV2, in the final nn.Linear layer why is bias kept?

[Low Level Technicals of LLMs: Daniel Han](https://www.youtube.com/watch?v=pRM_P6UfdIc)

Biased vs Unbiased variance - Bessel's correction
