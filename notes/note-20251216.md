# Summary

**Feed forward networks**

- We started with simple feed forward network to do classify digits in MNIST dataset.
- This model has roughly 80K parameters. This model acheives 97% test accuracy.

**Convolution networks**

- Learned how to stack convolutional layers with batchnorm and relu and max pooling layers
- We transform the input image from a single channel into 16 channels using convolution kernel
- formula of dimension after convolution is

  New Dimension = Floor((OldDimension + 2 \* Padding - KernelSize) / Stride) + 1

- Pick (Kernel = 3, Padding = 1, Stride = 1)), (Kernel = 5, Padding = 2, Stride = 1)) to keep the input and output dimension the same.

- Downsampling can be done via MaxPool2D layer

- Since we are already at 97-98% accuracy. We don't see that many real wins in accuracy here.
- We still have not made any attempt at data augmenation. but it will be hard to see real wins on this simple dataset.

- The size of convolution net is around 20K parameters. So we are achieving similar accuracy with network 1/4 the size.

- Instead 1 -> 16 - 32 channels, 1 -> 8 -> 16 channels (~10K parameters), we still maintain the same accuracy

- We can make the model much smaller without hurting their accuracy.

**CIFAR - 10**

- This dataset uses 32x32 images from 10 classes
- We started with same conv net from MNIST with 60% accuracy.
- Data augmentation + minor changes to model pushed the accuracy all way up to 85%
- Moving to a resnet architecture gave about 94% accuracy on test data

## Todos

- [ ] Study the distribution of wieghts and activations for different models
